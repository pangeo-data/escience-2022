{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {},
   "source": [
    "# Data chunking and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281bb79b-1b06-42c9-98d5-6185252c86e5",
   "metadata": {},
   "source": [
    "## Authors & Contributors\n",
    "### Authors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)\n",
    "### Contributors\n",
    "- Pier Lorenzo Marasco, Ispra (Italy), [@pl-marasco](https://github.com/pl-marasco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> Overview\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>Why do chunking and compressing matter?</li>\n",
    "        <li>How can I chunk datasets to optimize memory usage?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about chunking and compression</li>\n",
    "        <li>Use kerchunk to chunk and prepare datasets for parallel computing</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "\n",
    "We will be using [kerchunk library](https://fsspec.github.io/kerchunk/) to chunk and access datafiles to understand chunking with Xarray to analyze large datasets. The analysis is very similar to what we have done in previous episodes but this time we will use data on a global coverage and not only on a small geographical area (e.g. Lombardia).\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using Global Long Term Statistics (1999-2019) product provided by the [Copernicus Global Land Service over Lombardia](https://land.copernicus.eu/global/index.html) and access them through [S3-comptabile storage](https://en.wikipedia.org/wiki/Amazon_S3) ([OpenStack Object Storage \"Swift\")](https://wiki.openstack.org/wiki/Swift) with a data catalog we have created and made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c600794-dd9e-400b-bd09-dbb6e7039dad",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following Python packages:\n",
    "\n",
    "- fsspec\n",
    "- s3fs\n",
    "- pooch\n",
    "- xarray\n",
    "- netcdf4\n",
    "- h5netcdf\n",
    "- hvplot\n",
    "- kerchunk\n",
    "- pprint\n",
    "- json\n",
    "- geopandas\n",
    "- matplotlib\n",
    "\n",
    "Please install these packages if not already available in your Python environment. Below, we only install packages that are not available in the EGI-ACE EOSC deployment of Pangeo for the FOSS4G course.\n",
    "\n",
    "### Packages\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4818f-2b6b-44c1-af77-97daf8a1c2a1",
   "metadata": {},
   "source": [
    "## Global LTS\n",
    "\n",
    "In the previous episode, we used Long-term Timeseries for the region of Lombardy e.g. a very small area. Now we would like to use the original dataset that has a global coverage. Let's first open one single file (for January 1999-2019) to understand how much larger the global dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a418ed-c764-4577-803c-a6947db16966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import s3fs\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bdead9-5f24-442e-93bf-715068fd330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True,\n",
    "      client_kwargs={\n",
    "         'endpoint_url': 'https://object-store.cloud.muni.cz'\n",
    "      })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9aeebc-db66-4469-8b3b-7b79fa24148d",
   "metadata": {},
   "source": [
    "As soon as we have several files, we use Xarray `open_mfdataset` instead. In that case, you need to pass a list and not a single element. We can also use `open_mfdataset` with one file. For this we need to pass a list with one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a41ca7-ccd2-4223-ae15-d4119d7657cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = 's3://foss4g-data/CGLS_LTS_1999_2019/c_gls_NDVI-LTS_1999-2019-1221_GLOBE_VGT-PROBAV_V3.0.1.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af670e00-fbad-4f70-a24f-1b6dc7320a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS = xr.open_mfdataset([fs.open(s3path)])\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d2938-d0ea-41fe-b32c-af81a8055416",
   "metadata": {},
   "source": [
    "## Chunking and compression\n",
    "\n",
    "You may have missed it but if you look carefully to `LTS`, each Data Variable is a `dask.array` with a chunsize of `(15680, 40320)`. So basically accessing one data variable would load at least `(15680, 40320)` into the memory.\n",
    "\n",
    "By default, the chunks correspond to the entire size of the dataset. When you need to analyse large number of large files, the memory may not be sufficient anymore. This is where understanding chunking comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f644ba3-8eb5-4cf9-bd20-0aebda8bc05e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LTS.sel(lat=slice(80.,70.),lon=slice(70.,90))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231226cf-027b-4f6a-8558-415e182336bd",
   "metadata": {},
   "source": [
    "When selecting you may have the feeling the chunk sizes changes. In fact Xarray will still have to fetch the entire initial chunk to perform the selecting on any of the Data variables. Again it won't be very optimal (your Python Jupyter kernel may crash too!) with large number of files and large files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29025fb7-3fc0-499e-9117-9377a0899f67",
   "metadata": {},
   "source": [
    "## Exploiting native chunking for reading datasets\n",
    "\n",
    "Many data formats (for instance, [netCDF4](https://unidata.github.io/netcdf4-python/), [HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) or [geoTIFF](https://en.wikipedia.org/wiki/GeoTIFF)) chunk natively the datasets (done at the creation of the datasets). These native chunks can be retrieved and used when opening and accessing the files. This will allow to significantly reduce the amount of memory when analyzing Data Variables (only the needed chunks will be transfered and if all need to be accessed, it can be serializable e.g. chunks are processed one after the other). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae359620-527f-4525-9cba-3b8cc6754750",
   "metadata": {},
   "source": [
    "### Extract chunk information\n",
    "We extract chunk information from each file using kerchunk.hdf. Let's first with one file.\n",
    "\n",
    "Make sure you have install kerchunk (version 0.07) if not already available in your environemnt.\n",
    "```\n",
    "pip install kerchunk==0.0.7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a382f-d924-40bd-a3bb-a82a7dcc0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerchunk.hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6767b56-d17b-48f4-9718-ee87207e98c0",
   "metadata": {},
   "source": [
    "We use `kerchunk.hdf` because our files are `netCDF4` (based on HDF5) and `SingleHdf5ToZarr` to translate the content of one HDF5 file into Zarr metadata. The parameter `inline_threshold` is an *optimization* and tells `SingleHdf5ToZarr` to include chunks smaller than this value directly in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdce787-38f6-4dc1-99a7-31e6e3925d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remote_filename = 'https://object-store.cloud.muni.cz/swift/v1/foss4g-data/CGLS_LTS_1999_2019/c_gls_NDVI-LTS_1999-2019-1221_GLOBE_VGT-PROBAV_V3.0.1.nc'\n",
    "with fsspec.open(remote_filename) as inf:\n",
    "    h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, remote_filename, inline_threshold=100)\n",
    "    chunk_info = h5chunks.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7d185-8efa-40cb-8625-a5589b32bed4",
   "metadata": {},
   "source": [
    "Let's have a look at `chunk_info`. It is a Python dictionary so we can use `pprint` to print it nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eacc19-c075-492b-8efc-fefe735b591b",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(chunk_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f73e1-05a6-46eb-a867-6b0f49a81df1",
   "metadata": {},
   "source": [
    "After we collected information on the native chunks in the original data file, we can open the files using `zarr` and pass this chunk information as a storage option. We also need to pass `\"consolidated\": False` because the original dataset does not contain any `zarr` consolidating metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c82da0-e286-463d-a801-04a651102029",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\": chunk_info,\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae4b55-df4f-4829-9142-e06c2b910c1f",
   "metadata": {},
   "source": [
    "As you can notice above, all the Data Variables have several chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f407f0-f9b8-4320-bed1-5b77c14949fb",
   "metadata": {},
   "source": [
    "### Combine all LTS files into one  kerchunked catalog\n",
    "Now we will combine all the files into one kerchunked catalog, and try to open it as a xarray dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceabe66-09bc-42a8-9469-edabd88791bf",
   "metadata": {},
   "source": [
    "Let's first collect the chunk information for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd3546-5431-4d64-9f23-24cfe90489a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls('foss4g-data/CGLS_LTS_1999_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe5c6a-4471-4481-8a61-4e17bdd4d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e76758-25fb-49f4-86c5-f8f18275fb81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3path = 's3://foss4g-data/CGLS_LTS_1999_2019/c_gls_*.nc'\n",
    "chunk_info_list = []\n",
    "time_list = []\n",
    "\n",
    "for file in fs.glob(s3path):\n",
    "    url = 'https://object-store.cloud.muni.cz/swift/v1/' + file\n",
    "    t = datetime.strptime(file.split('/')[-1].split('_')[3].replace('1999-', ''), \"%Y-%m%d\")\n",
    "    time_list.append(t)\n",
    "    with fsspec.open(url) as inf:\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, url, inline_threshold=100)\n",
    "        chunk_info_list.append(h5chunks.translate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf9ea0-c8c8-4fe9-a753-7558f587fb76",
   "metadata": {},
   "source": [
    "This time we use `MultiZarrToZarr` to combine multiple kerchunked datasets into a single logical aggregated dataset. Like when opening multiple files with Xarray `open_mfdataset`, we need to tell `MultiZarrToZarr` how to concatenate all the files. There is no time dimension in the original dataset but one file corresponds to one date (average over the period 1999-2019 for a given period of about 10 days e.g. January 01, January 11, January 21, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a232d-01c6-4590-af85-63790df41024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kerchunk.combine import MultiZarrToZarr\n",
    "mzz = MultiZarrToZarr(\n",
    "    chunk_info_list,\n",
    "    coo_map={'INDEX': 'INDEX'},\n",
    "    identical_dims=['crs'],\n",
    "    concat_dims=[\"INDEX\"],\n",
    ")\n",
    "\n",
    "out = mzz.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ef1e5-4fde-4f73-8203-9481528603c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\": out,\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844f7b7-4e7a-4d52-a4bb-076bb71226fd",
   "metadata": {},
   "source": [
    "We can save the catalogue in a file, and load the data from the catalog in your local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec340712-b9b6-48a4-be9a-37dd30b8f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8f485-230f-491a-a964-0b7357507734",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile='c_gls_NDVI-LTS_1999-2019.json'\n",
    "with open(jsonfile, mode='w') as f :\n",
    "    json.dump(out, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a80c9-2cfb-4ccf-8df1-4704874689a4",
   "metadata": {},
   "source": [
    "We can then load data from this catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0faea-66fb-4210-b119-bbbeeec47560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "LTS = xr.open_mfdataset(\n",
    "    \"reference://\", engine=\"zarr\",\n",
    "    backend_kwargs={\n",
    "        \"storage_options\": {\n",
    "            \"fo\":'./c_gls_NDVI-LTS_1999-2019.json',\n",
    "        },\n",
    "        \"consolidated\": False\n",
    "    }\n",
    ")\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880c571-92c5-4b25-8a1c-0bad61b6175e",
   "metadata": {},
   "source": [
    "We can also postprocess LTS to get the dates properly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f1e82-8041-46ff-95f1-f3e82d81cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTS = LTS.rename({'INDEX': 'time'}).assign_coords(time=time_list)\n",
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ffd88b-6eb8-45d9-ad26-fdb3ca7af507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d1165-7b57-45fd-b9ed-05046ce286c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    GAUL = gpd.read_file('Italy.geojson')\n",
    "except:\n",
    "    GAUL = gpd.read_file('zip+https://mars.jrc.ec.europa.eu/asap/files/gaul1_asap.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363bafb-551a-4a25-8793-b854cefc071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AOI_name = 'Lombardia'\n",
    "AOI = GAUL[GAUL.name1 == AOI_name]\n",
    "AOI_poly = AOI.geometry\n",
    "AOI_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe014a-8bd8-4b9e-8a4b-c3cdda63833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTS = LTS.sel(lat=slice(46.5,44.5), lon=slice(8.5,11.5))\n",
    "LTS = LTS.rio.write_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbfab2-a11a-46f2-858c-4c336ad76bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTS = LTS.rio.clip(AOI_poly, crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503edd2-cfc7-4f8c-a06e-481e63527ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35969623-39a1-4ea2-bf4b-136b2225ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b29622-c979-4e42-873a-366d4660e313",
   "metadata": {},
   "source": [
    "## Visualize LTS statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb92583-615c-4e5b-b773-21da14c932fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad179317-353b-4bcd-9b1d-604572a2129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=[15,5])\n",
    "\n",
    "# Fix extent\n",
    "minval = 0.0\n",
    "maxval = 0.9\n",
    "\n",
    "itime=0 # plot the first date\n",
    "\n",
    "# Plot 1 for min subplot argument (nrows, ncols, nplot)\n",
    "# here 1 row, 2 columns and 1st plot\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "LTS.isel(time=itime)['min'].plot(ax=ax1)\n",
    "# Plot 2 for max\n",
    "# 2nd plot \n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "LTS.isel(time=itime)['max'].plot(ax=ax2)\n",
    "\n",
    "# Title for both plots\n",
    "fig.suptitle('LTS NDVI statistics (Minimum and Maximum)', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509a57d-2960-48f9-a9bb-2e163477c83e",
   "metadata": {},
   "source": [
    "The catalog (json file we created) can be shared on cloud (or github, etc.) and anyone can load it from there too.\n",
    "This approach allows anyone to easily access LTS and select the Area of Interest for their own study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665722c-4c65-4334-987e-1f5bc4841036",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Understanding chunking is key to optimize your data analysis. In this episode we learned how to optimize the memory by exploiting native chunks from netCDF4 data files and instructing Xarray to access data per chunk. However, computations can be very slow and to optimize the computational part we need to parallelize our data analysis. This is what you will learn in the next episode with Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1f23a-8838-41fe-96d7-4f7b0fb9cc3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Key Points</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Chunking and compression</li>\n",
    "        <li>kerchunk</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
